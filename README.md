# Neural Network (TCN) Time-Series ICU Deterioration Predictor (LightGBM & PyTorch) ðŸ“ˆðŸ“Š 

Python-based ICU deterioration predictor leveraging advanced time-series ML modeling using TCN Neural Network (PyTorch) on timestamp-level vitals and LightGBM on patient-level features, capturing temporal trends, missingness, and sequence dynamics. 

Benchmarked against NEWS2, with real-time alerts and dual CLI/FastAPI deployment for real-time monitoring.

Portfolio-ready, deployed, and clinically-informed.

**Tech stack**: python, pandas, NumpPy, LightGBM, PyTorch

**Pipeline**
```text
Raw ICU Vitals (long format, MIMIC-style)
   â””â”€> compute_news2.py
         â”œâ”€ Input: raw vitals CSV
         â”œâ”€ Action: compute NEWS2 scores per timestamp
         â””â”€ Output: news2_scores.csv (wide format with vitals, NEWS2 score, escalation labels), news2_patient_summary.csv (patient-level summary)

news2_scores.csv
   â””â”€> make_timestamp_features.py
         â”œâ”€ Action:
         â”‚   â”œâ”€ Aggregate per patient
         â”‚   â”œâ”€ Add missingness flags
         â”‚   â”œâ”€ Apply LOCF per vital
         â”‚   â”œâ”€ Compute carried-forward flags
         â”‚   â”œâ”€ Compute rolling window stats (1h/4h/24h)
         â”‚   â”œâ”€ Compute time-since-last-observation
         â”‚   â””â”€ Encode risk/escalation as ordinal numeric
         â””â”€ Output: news2_features_timestamp.csv
               (ML-ready timestamp-level features)

news2_scores.csv
   â””â”€> make_patient_features.py
         â”œâ”€ Action:
         â”‚   â”œâ”€ Aggregate per patient
         â”‚   â”œâ”€ Compute median, mean, min, max, std per vital
         â”‚   â””â”€ Include % missingness per vital
         â””â”€ Output: news2_features_patient.csv
               (ML-ready patient-level summary features)
```

# Timestamp features rationale
- We compute rolling window features over 1h, 4h, and 24h intervals. 
   - Mean, min, max, and std capture the magnitude and variability of vitals. 
   - Slope gives the trend â€” whether the vital is rising or falling and how fast. 
   - AUC measures cumulative exposure, i.e., how much and for how long a patient has experienced abnormal values. 
- These features provide temporal context for the ML model, so it doesnâ€™t just see isolated values but also their trajectory over time.


# LightGBM vs Neural Network (TCN) Pipeline
```text
ML Model (LightGBM)
   â”œâ”€ Input: news2_features_patient.csv 
   â”‚     â”œâ”€ Median, mean, min, max per vital
   â”‚     â”œâ”€ Impute missing values
   â”‚     â”œâ”€ % missing per vital
   â”‚     â””â”€ Risk summary stats (max, median, % time at high risk)
   â”œâ”€ Action:
   â”‚     â”œâ”€ Train predictive model for deterioration / escalation
   â”‚     â”œâ”€ Use timestamp trends + missingness flags
   â”‚     â””â”€ Evaluate performance (AUROC, precision-recall, etc.)
   â””â”€ Output: predictions, feature importances, evaluation metrics

ML Model (Neural Network, TCN)
   â”œâ”€ Input: news2_features_timestamp.csv
   â”‚     â”œâ”€ Timestamp-level vitals & rolling features (mean, min, max, std, slopes, AUC)
   â”‚     â”œâ”€ Missingness flags
   â”‚     â”œâ”€ Carried-forward flags  
   â”‚     â””â”€ Time since last observation
   â”œâ”€ Action:
   â”‚     â”œâ”€ Train predictive model for deterioration / escalation
   â”‚     â”œâ”€ Learn temporal patterns, trends, and interactions
   â”‚     â”œâ”€ Can handle sequences of variable length per patient
   â”‚     â””â”€ Evaluate performance (AUROC, precision-recall, calibration)
   â””â”€ Output: 
         â”œâ”€ Predictions per timestamp or per patient
         â”œâ”€ Learned feature embeddings / attention weights (if applicable)
         â””â”€ Evaluation metrics
```

# LightGBM vs Neural Network (TCN) Pipeline Visualisation
```text
  Raw EHR Data (vitals, observations, lab results)
         â”‚
         â–¼
Timestamp Feature Engineering (news2_scores.csv)
 - Rolling statistics (mean, min, max, std)
 - Slopes, AUC, time since last observation
 - Imputation & missingness flags
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º TCN Neural Network Model (v2)
         â”‚              - Input: full time-series per patient
         â”‚              - Can learn temporal patterns, trends, dynamics
         â”‚
         â–¼
Patient-Level Feature Aggregation (make_patient_features.py â†’ news2_features_patient.csv)
 - Median, mean, min, max per vital
 - % missing per vital
 - Risk summary stats (max, median, % time at high risk)
 - Ordinal encoding for risk/escalation
         â”‚
         â–¼
LightGBM Model (v1)
 - Input: one row per patient (fixed-length vector)
 - Uses aggregated statistics only
 - Cannot handle sequences or variable-length time series
```

 # Model Comparison: LightGBM vs Neural Network (V1 & V2)

```text
| Aspect | LightGBM (V1) | Temporal Convolutional Network (TCN) (V2) |
|--------|-------------------|-------------------|
| **ML Model Name / Type** | LightGBM (Gradient Boosted Decision Trees) | Temporal Convolutional Network (TCN)(Neural network) |
| **V1 / V2** | V1: uses patient-level features, baseline interpretable patient summary (classic tabular ML) | V2: uses timestamp-level features, advanced sequence modeling (modern deep learning) |
| **Input Datasets** | `news2_features_patient.csv` (patient-level summaries) | `news2_features_timestamp.csv` (time series of vitals, missingness flags) |
| **Optional Inputs** | Timestamp features could be added later for hybrid model | Patient-level summary features from `news2_features_patient.csv` can be appended but not mandatory |
| **Reason for this input choice** | LightGBM is a tree-based model: handles static features and aggregates well; does not naturally model temporal sequences | Neural networks (LSTM/TCN) can model temporal trends, sequences, and interactions over time; need full timestamp features to exploit sequential information |
| **Why two different models** | 1. LightGBM: fast, interpretable (feature importance), strong baseline.<br>2. Neural network: captures temporal dynamics, can potentially improve predictive performance on time-series deterioration | Complements LightGBM; addresses potential limitations of static patient summaries by using sequential information in timestamp features |
| **Strengths** | - Handles missing values gracefully.<br>- Fast training and inference.<br>- Provides feature importances.<br>- Works well with tabular summary features. | - Models temporal trends and interactions.<br>- Can capture subtle patterns in sequences of vitals.<br>- Potentially better performance on real-time deterioration prediction. |
| **Weaknesses / Limitations** | - Ignores sequence and timing of events.<br>- May lose some granularity of patient trajectory.<br>- Cannot capture interactions over time. | - Requires more computation and tuning.<br>- Harder to interpret.<br>- Sensitive to missing data; requires careful imputation or masking. |
| **Output** | Predictions per patient, feature importances, evaluation metrics (AUROC, PR-AUC, etc.) | Predictions per timestamp or per patient trajectory, evaluation metrics (AUROC, PR-AUC, potentially time-dependent metrics) |
| **Use case / Deployment** | Baseline model; interpretable; fast deployment; can be used for early warning systems using summary features | Advanced model for final deployment or v2 experimentation; may be integrated in real-time monitoring dashboards for continuous deterioration prediction |
```